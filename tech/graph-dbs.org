#+SETUPFILE: https://fniessen.github.io/org-html-themes/setup/theme-readtheorg.setup
#+STARTUP: showall
#+EXPORT_FILE_NAME: /var/tmp/graph-dbs.html
#+OPTIONS: ^:nil

* WHY Graphs?
- A graph db is a recommendation for problems that require multiple joins on different table in SQL.
  - SQL is too low level, 3NF, killed by joins
- NoSQL db have
  - No Schema
  - No Constraints
  - too level

* [[https://docs.aws.amazon.com/neptune/latest/userguide/intro.html][AWS Neptune]]
** Highlights from [[https://docs.aws.amazon.com/neptune/latest/userguide/intro.html][user guide]]
- fast, reliable, fully managed graph database service
- high-performance graph database engine that is optimized for storing billions of relationships and querying the graph with milliseconds latency.
- supports the popular graph query languages Apache TinkerPop Gremlin and W3Câ€™s SPARQL, allowing you to build queries that efficiently navigate highly connected datasets.
- Neptune powers graph use cases such as recommendation engines, fraud detection, knowledge graphs, drug discovery, and network security.
- Neptune is highly available, with read replicas, point-in-time recovery, continuous backup to Amazon S3, and replication across Availability Zones. Neptune provides data security features, with support for encryption at rest and in transit. Neptune is fully managed, so you no longer need to worry about database management tasks like hardware provisioning, software patching, setup, configuration, or backups.
** configurations
- [[https://docs.aws.amazon.com/neptune/latest/userguide/iam-auth-connecting-gremlin-console.html][Connecting to Neptune Using the Gremlin Console with Signature Version 4 Signing]] from local
- [[https://docs.aws.amazon.com/neptune/latest/userguide/access-graph-gremlin-console.html][Set up the Gremlin Console to Connect to a Neptune DB Instance]] from ec2 (easy)
** Data transfer
- [[https://docs.aws.amazon.com/neptune/latest/userguide/bulk-load.html][Loading data into Neptune]]
- [[https://docs.aws.amazon.com/neptune/latest/userguide/bulk-load-tutorial-format.html][Neptune Load Formats]]


* Titan on AWS
** [[https://aws.amazon.com/blogs/startups/amazon-dynamodb-storage-backend-for-titan-distributed-graph-database/#][Aws Dynamo DB Storage backend for titan graph]]
*** Nice way to build the graph
*** Didn't prototype this and the reason is that it needs some initial configurations which might take sometime
*** two different appraches
**** single item model
**** multi item model (may the right fit for us)


* Janus Graph
** TODO Third Party Providers
*** Run cloud formation to create the AWS Dynamodb backend for Janus Graph. [[https://github.com/awslabs/dynamodb-janusgraph-storage-backend][link]]
/this project is not maintained anymore skip this step/
The cloud formation for the gremlin server there has some issues. Follow the steps here. [[https://bricaud.github.io/personal-blog/janusgraph-running-on-aws-with-dynamodb/][link]]

** Schema and Data Modeling. [[https://docs.janusgraph.org/basics/schema/][link]
- comprises of vertex labels, edge labels and property keys
- can be explicity or implicity defined. explicity defining the schema is good as it helps in keeping the graph robust.
- extending the schema doesn't slow down query answering or database down time.
- Defining edge labels
  - edge label names must be unique in the database
  - the multiplicity of the edge label determines the multiplicity constraint on all the edges of this label. (a maximum number of edges between pairs of vertices)
  - Multiplicity Settings
| Multipliciy     | example     |
|-----------------+-------------|
| MULTI (default) | transaction |
| SIMPLE          | friend      |
| MANY2ONE        | mother      |
| ONE2MANY        | winner of   |
| ONE2ONE         | married to  |
 - Defining property keys
   - use data type class to determine the data type of a property key. Janus graph will enforce that all the values associate with the key have configured data type thereby ensures the data added to the graph is valid.
   - you can define your own data types too. refer to section 5.3.1
   - use cardinality to define the allowed cardinality of the values associated with the key on any given vertex.
 - edge labels and property keys are together referred as relationship types.
   - names of relationship types should be unique in the graph, which means that edge labels and property keys cannot have the same name.
| NAME      | DESCRIPTION                        |
|-----------+------------------------------------|
| String    | Character Sequence                 |
| Character | Individual Character               |
| Boolean   | true or false                      |
| Byte      | Byte value                         |
| Short     | Short Value                        |
| Integer   | Interger Value                     |
| Long      | Long Value                         |
| Float     | 4 byte floating point number       |
| Double    | 8 byte floating point number       |
| Geoshape  | Geoshape like point, circle or box |
| UUID      | Universally unique identifier      |

| Cardinality      | Description                                                             | Example         |
|------------------+-------------------------------------------------------------------------+-----------------|
| SINGLE (default) | Allows one value per element                                            | date of birth   |
| LIST             | Allows an arbitary number of values per element                         | sesnor readings |
| SET              | Allows multiple values but no duplicate values per element for such key |                 |
 - Defining vertex labels
   - Like edges vertices have labels. Unlike edges vertex labels are optional. they are useful to distinguish different types of verticies. ex: user vertex and product vertex.
   - Although labels are optional at the conceptual and data model level, janus graph assigns all vertices a label ass an internal implementation detail. vertices created by the addVertex methods use janus graph's default label.
   - Vertex labels are unique in a graph.
 - Automatic Schema generation ( avoid at all costs )
   - when an undefined edge label, property key or vertex label is first used it is defined implicitly.
   - default settings are used when defining something implicitly.
   - user can control automatic schema creation.
 - Changes to Schema ( be extremely careful and take precautions given in the docs )
   - the definition of a property key, edge label or vertex label cannot be changed once committed to the graph, however the names of the schema elements can be changed.
   - certain things can go wrong during this process. refer to section 5.7
   - redefining the existing schmea can be done by changing the name of this type to a name that is currently (or will never be) in use. After that a new label or key can be defined with the original name thereby replacing the origial one. this will not effect the vertices, edges and properties previously defined with existing type.
   - Redefining existing graph elements is not supported online and must be accomplished through batch processing.
 - Schema Constraints
   - Allows users to define which 2 vertex labels can be connected by a edge label.
   - Can be used to make sure the graph matches a domain model.
   - schema constraints can be enabled by setting schema.constraints=true.
     - depends on schema.default, if none then an illegal argument exception is thrown for constraint violations else schema constraints are automatically created, but no exception is thrown.
     - activating it has no impact on the existing data.
     - these are only applied during the insertion process, therefore reading data is also not affected.
** Janus Graph Query Language - GREMLIN
** Configured Graph Factory. [[https://docs.janusgraph.org/latest/configuredgraphfactory.html#overview][chapter 9]]

*** how is it similar & different to janus graph factory?
- JGF provides access point to your graph by providing a configuration object each time you access the graph. CGF provides an access point to your graph for which you have previously created configurations. CGF also offers access point to manage graph configurations.
- CGF allows you to manage graph configurations.
- JanusGraphManager is an internal server component that tracks graph references provided your graphs are configured to use.
- CGF can only be used if you have configured your server to use ConfigurationGraphManagement API's at server start.
- Benefits of CGF
  - only need to supply a string to access your graph as supposed to JGF which requires you to specify the backend you wish each time you connect to the graph.
  - If your CGF is configured with a distributed storage backend then your graph configurations are available to all janus graph nodes in the cluster.

*** How does CGF work?
Provides access points in 2 scenarios.
- You have already created a configuration for your specific graph object using the ConfigurationManagementGraph#createConfiguration. In this case your graph is opened with previously created configuration for your graph.
- You have created a template configuration using ConfigurationManagementGraph#createTemplateConfiguration. In this scenarion, we create a configuration for the graph you are creating by copying over all the attributes stored in your template configuration and appending the relevant graphName attribute, and we can open the graph according to the specific configuration.

*** Graph Factory
- Assuming that gremlin server started successfully and ConfigurationManagementGraph was successfully instantiated, then all the apis available on the ConfigurationGraphManagement Singleton will also act upon said.
- Further more this is the graph that will be used to access the configurations used to create/open graphs.
- The ConfigurationManagement is a SingleTon that allows you to create/update/remove configurations that you can access your graphs using the ConfiguredGraphFactory

**** Accessing the graphs

- ConfiguredGraphFactory.create('graphName')
- ConfiguredGraphFactory.open('graphName')
- ConfiguredGraphFactory.getGraphNames() // JanusGraphFactory.getGraphNames()
- ConfiguredGraphFactory.drop('graphName')

*** Configuring janus graph server  [[https://docs.janusgraph.org/latest/server.html#first-example-connecting-gremlin-server][Chapter 7]] [[https://docs.janusgraph.org/latest/deployment-scenarios.html][8]] [[https://docs.janusgraph.org/latest/configuredgraphfactory.html][9]].

To be able to use the CGF you must configure your server to use ConfigurationGraphManagement API's. To do this you have to inject a graph variable named " ConfiguredManagementGraph " in your server's YAML's graphs map.

For example
#+BEGIN_SRC
graphManager: org.janusgraph.graphdb.management.JanusGraphManager
graphs: {
  ConfigurationManagementGraph: conf/JanusGraph-cql-configurationmanagement.properties
}
#+END_SRC

In this example, our ConfigurationManagementGraph will be configured using the properties stored inside conf/JanusGraph-configurationmanagement.properties which for example looks like
#+BEGIN_SRC
gremlin.graph=org.janusgraph.core.ConfiguredGraphFactory
storage.backend=cql
graph.graphname=ConfigurationManagementGraph
storage.hostname=127.0.0.1
#+END_SRC

*IMP:*
- the file at //scripts/empty-sample.groovy// binds the traversal objects to the global variables so they are available when someone connects to the gremlin server remotely.
  - This is one file that should not be replaced before we actually configured the custom graph and added the schema.
    - use the file scripts/xxx-setup.groovy to configure and setup the graph with schema
- However the graph has defined prior to starting the server with the file from the repo as it also contains the g traversal which is a binding to the configured-graph.
- in the empty-sample.groovy file please the line to the list trav: ConfiguredGraphFactory.open('graphname')

**** Example of creating a new graph.
- janus-graph is not configured to use configured janus-graph factory by default. Follow the instructions [[https://stackoverflow.com/questions/51594838/janusgraph-please-add-a-key-named-configurationmanagementgraph-to-the-graph][here]] to make it the default.

#+name: creating a new graph
#+BEGIN_SRC gremlin
// adding cassandra storage-backend & elasticsearch-backend
map = new HashMap();
map.put("storage.backend", "cql");
map.put("storage.hostname", "127.0.0.1");
// this below line adds the graph name
map.put("graph.graphname", "ikg");
map.put("index.search.backend", "elasticsearch");
map.put("index.search.hostname", "127.0.0.1");
map.put("index.search.elasticsearch.transport-scheme", "http");
ConfiguredGraphFactory.createConfiguration(new MapConfiguration(map));
#+End_SRC

*** Connecting to the graph from gremlin python
**** server side (not intended for users)
- install drivers for python on the janus gremlin server (i think this is no longer necessary in the newer version)
  - stop the janus server if already running
  - run ./bin/gremlin-server.sh -i org.apache.tinkerpop gremlin-python 3.4.1
  - the version of the gremlin-python is the version of the apache tinker pop that is compatible with the janusgraph verison
  - start the janus server
**** client side
- install gremlinpython lib in the python virtual env. The version of the lib should be same as that of thinkerpop compatible with janus graph version.
- follow the [[https://docs.janusgraph.org/latest/connecting-via-python.html][steps]] from the documentation.
- Good to know
  - DriverRemoteConnection takes in several parameters to make the connection.
    #+BEGIN_SRC
    class DriverRemoteConnection(RemoteConnection):

    def __init__(self, url, traversal_source, protocol_factory=None,
                 transport_factory=None, pool_size=None, max_workers=None,
                 username="", password="", message_serializer=None,
                 graphson_reader=None, graphson_writer=None):
    #+END_SRC
    - traversal source is the "/trav/" in our case.
    - if you want to play with the graph use the traversal source "/gods/" which connects to the traditional graph of gods

** Bulk Loading Data [[https://docs.janusgraph.org/latest/bulk-loading.html][chapter 37]]

*** Use cases

- Moving into janusgraph from another environment.
- use janus graph as an etl endpoint
- adding external datasets
- updating janus graph with results of graph analytics job

*** TODO configurations

- enabling storage.batch-loading will have the biggest +ve impact as it will disable the checks in number of places and assumes the data is consistent.
- Avoiding ID allocation conflicts. May be not a problem when janus is running on a single instance. ID allocation conflicts are inevitable when id blocks are frequently allocated by janus-graph.
  - ids.authority.wait-time
  - ids.renew-timeout
- optimizing storage, writes and reads (be very careful, read more before experimenting)
  - storage.buffer-size
  - storage.read-attempts
  - storage.write-attempts
  - storage.attempt-wait
- bulkloading strategies
  - todo

** Indexing
[[https://docs.janusgraph.org/basics/index-performance/]]
*** understanding indexes [[https://github.com/JanusGraph/janusgraph/wiki/Indexing][here]]
*** creating indexes
Most graphs start their operations from a list of vertices or edges that are identified by their properties. Supports 2 different types of indexes
- graph index
  - make global retervial operations efficient on large graphs
- vertex centric index
  - speeds up actual traversal through the graph

Janus graph distinguishes between 2 types of indexes.
- Composite
  - these are very fast and efficient but limited to equality conditions, for a particular previously defined combination of vertex keys
- Mixed
  - Can be used for look ups on any combination of indexed keys and support multiple condition predicates in addition to equality depending on the backing index store

Indexes are created through janus graph management system and index builder returned by JanusGraphManagement.buildIndex(String, class) where 1st arg is the name of the index and the second arg is the type of element to be indexed on (eg. Vertex.class)
- The name of the graph index in unique.
- Graph indexes built against newly defined property keys (keys that are defined  in the same mgmt transaction as index) are available immediately.
- Same applies to graph indexes that are constrained to a label.
- Graph indexes built against property keys that are already in use without being constrained to a newly created label requires a execution of reindex procedure to ensure the index contains all the previously added elements. Index will be available after the reindex is completed.
  - it is good to define graph indexes in the same transaction as the schmea but when bulk loading this not advisable.
*** removing index [[https://docs.janusgraph.org/latest/index-admin.html#mr-index-removal][chapter 36.2]]
Remove index consists of 2 stages.
- changing the index state to disabled. JanusGraph signals to all others via the storage backend that the index is slated for deletion.
  - at this point janus graph stops using the index
  - index-related data in the backend remains stale but ignored.
- Depending on whether the index is mixed or composite
  - composite: A composite index can be deleted via JanusGraph.
  - mixed: A mixed index needs to deleted via the backend. JanusGraph currently (0.4.0) doesnot suport this.
*** stuck indexes
- stack overflow titan db ignoring index. [[https://stackoverflow.com/questions/40585417/titan-db-ignoring-index/40591478#40591478][here]]
** online help
 - google-groups
   - [[https://groups.google.com/forum/#!topic/janusgraph-users/fJTivKcGVV8][how to connect to janus graph of my choice]]
 - medium.com
   - [[https://medium.com/@BGuigal/janusgraph-python-9e8d6988c36c][JanusGraph & Python]]
 - stack-over flow
   - [[https://stackoverflow.com/questions/51594838/janusgraph-please-add-a-key-named-configurationmanagementgraph-to-the-graph][adding configuration management to graphs]]
   - [[https://stackoverflow.com/questions/49462588/custom-id-in-gremlin-python][custom id in graphs]]
   - [[https://stackoverflow.com/questions/40585417/titan-db-ignoring-index/40591478#40591478][here]]
 - others
   - [[https://kgoralski.gitbook.io/wiki/janusgraph]]





* Gremlin

** Documentation Notes

*** Getting started
- https://docs.janusgraph.org/basics/gremlin/

*** Crafted Machines
- Blueprints
- Pipes
- Frames
- Furnace
- Rexster


*** Advantages
- choosing a TinkerPop-enabled graph and using Gremlin in the correct way when building applications shields them from change and disparity in the space.
- As a graph provider, choosing to become TinkerPop-enabled not only expands the reach their system can get into different development ecosystems, but also provides access to other query languages through bytecode compilation
- Gremlin is Gremlin is Gremlin.
  - It is same when written against an in-memory graph with OLTP or written against Big Graph with OLAP.
  - It is same either we write it in java or javascript or python or console. Syntactial differences might be language specific. ex. lambda in groovy is different from that in python.

** Blogs

*** Pipes
LINK: https://dzone.com/articles/nature-pipes

**** Pipes

Pipes is a lazy evaluation framework in that as the pipeline is iterated for results, the mappings are computing as needed. One of the core methods of Pipe is List Pipe.getPath(). This method returns the transformations that an object underwent through a pipeline. Any gremlin expression has a string representation that exposes the underlying pipes being used to represent the expression.

3 types of Pipes
- transform: given an object of type S emit an object of type E.
- filter: Given an object of type S emit it or not.
- sideEffect: A side-effect pipe implements SideEffectPipe<S,T> and takes an object of type S and emits that object. However, in the process, it yields some computational side-effect that can for instance be used later in the traversal computation. The side-effect object id of type T. The example below makes  use do the most generic side-effect pipe: SideEffectClosurePipe. The example below calculates vertex 1's co-developeres -- that is, those vertices that have worked with vertex 1 on a project. Note that a vertex can not be their own co-developer and thus, sideEffect and filter are used appropriately to express this abstract co-developer and thus, sideEffect and filter are used appropriately to express this abstract co-developer relationship.

  g.V(1).sideEffect{x=it}.out('created').in('created').filter{it != x}.toString()

**** Branch and Meta Pipes
There are 2 pipe subtypes that should be discussed: branch and meta pipes. A branch pipe is any pipe that alters the flow of a computation based on some decision criteria. The ifThenElse step of gremlin demonstrates the behaviour of the closure-based ifThenElse Pipe.

The ifThenElse step take 3 closures: a condition-closure, a then-closure and finally, an else-closure. Other branch pipes include copySplit and the respective merging pipes which take multiple branches and yield a single flow: fairMerge and exhaustMerge.

The meta-pipes are perhaps the most bewildering yet most powerful of the pipes in Pipes. A meta-pipe is a pipe that maintains pipes internal to it. As such, the meta-pipe can reson on the results of its internal pipes and then effect an appropriate behavior at the meta-pipe level. To better explain the concept of MetaPipe, backtracking and looping are demonstrated.

g.V(1).out('knows').name.filter{it[0]=='v'}.back(2)

The above code snippet and diagram demonstrate the behaviour of back (BackFilterPipe). If an object reaches back, emit the object that it was n steps ago. In the example above, n is 2 and thus, two steps ago from the string vadas was vertex 2. While this linear sequence articulates backtracking, the diagram below better explains how this computation is implemented in Pipes.

g.V(1).out('knows') --> (2,4)
g.V(1).out('knows').name --> (vadas, josh)
g.V(1).out('knows').name.filter{it[0]=='v'} --> (vadas)
g.V(1).out('knows').name.filter{it[0]=='v'}.back(2) --> V(2)

It is also possible to use named-steps instead of integers to denote how many steps to back up to. As an expression becomes more complex, using named-steps makes the expression more readable and manageable.

LoopPipe is both a meta-pipe and branch pipe in that is can control the flow of objects and like BackPipe make use of internal pipes during its operations

g.v(1).out.loop(1){it.loops < 3}


* GRAKN

** Why Grakn

[[https://www.youtube.com/watch?v=bCxpNNzbz8M][from here]]
- SQL
  - too low level
  - 3NF (working at the logical level)
  - Queries can easily be killed by join
- Graph/No SQL
  - No Schema
  - No Constraints
  - lowel level data
- RDF/OWL
  - verbose
  - built for the web (open-world)
  - designed for experts of logic
  - low level data

Common Problems
- Could not model complex domains
- Could not simplify verbose queries
- Could not reuse analytical algorithms

Solution: Grakn & Graql, Grakn being the knowledge representation system and Graql is a query language.

|------------+------------------|
| Database   | work with        |
|------------+------------------|
| Relational | tables           |
| Graph      | nodes & edges    |
| Document   | Documents        |
| Grakn      | ER Diagram Model |
|------------+------------------|


** Knowledge Model
The knowledge schema represents
- type hierarchies
- hyper relations
- rules
[[file:images/grakn-knowledge-model.png]]

According to image we will be working with entities, attributes and relations which are called things. Things can play roles in relations.

** Services

*** Server
The knowledge server is where grakn stores the data in keyspaces.
*** Console
- The Grakn console interacts directly with a given keyspace that contains the grakn knowledge graph.

*** Workbase
- Connects to Grakn server and interacts with Keyspaces.
- Workbase allows us to use execute Graql get and compute ath queries, and visualise and investigate their results.
- A tool to test and experiment with your newly created Grakn Knowledge graph or that you prefer a graphical interface for reading data from Grakn.

*** Client
- Is an interface which we can use to read from and write to a Grakn Knowledge graph. If we are building an application that uses a Grakn knowledge graph as its database, we would need a Grakn client at our application layer to handle the database operations.
- All Grakn clients share a common architecture of which the main components are
  - *Client* is responsible for connecting to the Grakn Server. We then use the connection to manage keyspace and sessions
  - *Session* is responsible for connecting to our application to a particular keyspace. This connection would then allow opening transactions to carry out queries. We can think of a session as a two-way long-lasting tunnel that connects our application to a particular keyspace on the Grakn server.
  - *Transaction* is responsible for performing write and read operations over the types and instances with the connected keyspace. When executing a query to retrive data, an iterator is returned, which can then be lazily consumed to execute a request on the server to return the next concrete result.
- Futures and Async Queries
  - Queries can be computed asynchronously on the grakn serer whilst local processing takes place.
- Investigating Answers
  - Depending on the type of the query carried out by a transaction, we can retrieve different forms of answers. These answers, regardless of their type, all contain concepts. We can then use the methods introduced by the Concept API to obtain more information about the retrieved concept and its surroundings. Furthermore, the *Concept API* allows us to traverse the neighbours of a specific concept instance to obtain more insights
- Grakn accetps instructions and provides answers only in its own language - Graql..


** Graql
- Graql is the language for the Grakn knowledge graph.
- Whether it's through the Grakn Console, Workbase or one of the Grakn clients.

*** Schema
The Grakn schema sets the foundation for a Grakn knowledge graph. When modelled thoroughly, the schema provides us with a knowledge graph that benefits from logical integrity, is flexible towards change, capable of automated reasoning, and enables writing intuitive queries.

**** Why schema?
Schema is a means to address the problems of managing and handling unstructured or loosely structured data.

Problems with loosely structured data?

- Integrity
  - Lack of control over the validity of data.
  - No guarantee on data consistency and validity.
- Accessibility and retrieval
  - Lack of facilities for meaningful query answering.
  - Efficient querying is harder.
- Maintenance
  - Lack of data independence
  - Model changes and their propagation are harder.
- Extra App Logic
  - Pushes the responsibility to the user/developer.
  - Error prone and hard to maintain.

**** What is a Grakn Schema?
Grakn schema is an inherent part of the knowledge graph that describes how the data is and can be structured.

In the domain of schema-first database knowledge systems, database desings involves three schema:
- A high level conceptual schema, that models the problem and usually involves some variation of the entity-relation model.
- A mid level logical schema, that depends on the database type we are using. For example if we are going relational, this would involve turning the conceptual model into tables and going over a series of normalisation steps of the schema.
- A low level physical schmea, that requires us to optimise our schema according to how physical resources are distributed.

With Grakn, thanks to the high-level knowledge model, the schema closely resembles the conceptual schema, essentially avioding the hassle of going through the other two modelling steps. The grakn system takes case of this. This greatly simplifies the design process, providing us with, what can be considered, a highly normalised distributed schema with the need of going through the logical and physical modelling.

**** Grakn Data Model

***** Concepts
Everything that describes a domain in a Grakn knowledge graph is a concept. This includes the /elements of the schema/ (namely *types* and *roles*, which we call /schema concepts/) and the /actual data instances/. We refer to a data instance as a *thing* - they can be thought of as instance of types defined in the schema.

***** Types

Types consitute the core of the schema. They provide the necessary vocabulary to talk about our domain. They come in 3 flavours.
- Entities
- Relations
- Attributes

Apart from serving as a means of classification, types also define behaviours of their instances. Consequently, types also define the following behaviours:
- has [attribute type]
- plays [role]
- relates [role]

****** Entities
Entities are the main actors of our domain. These are usually the type of things we want to know about. Entity types provide means of classifying the objects in our domain

****** Relations
Relations connect other things together. Each relation can connect a number of things. A thing's participation in a relation is characterised by a *role* that can be played in that relation. Each relation is required to have at least one role.

****** Attributes
Attributes are used to characterise with small pieces of data (think of numbers, strings, dates etc.). Consequently, by defining attributes we can attach values of a specified value type to  our instances.

****** has [attribute type]
The ability to participate in relations of a specified type attached to an instance.

****** plays [role]
The ability to participate in relations that allow for that role to be played.

****** relates [role] (only relation)
The ability for other instances to play the given role instances of the defined relation type.

***** Type Hierarchies
Besides the modularity that the concept types provide, we are free to form subtype relationships between concept types. For a given child concept type that subtypes a parent concept type, the child concept type inherits the attributes owned by the and roles played by the parent type. The mechanism is analogous to subclassing in OOPs. Each concept type can have only a single parent type - multiple inheritance is not supported. Subtyping not only allows us to mirror the true nature of a dataset as perceived in the real world but also enables automated reasoning.

***** Roles
*Roles* specify the nature of the connection between instances. They are not types themselves. That means, we cannot have a thing which is an instance of a role, but we can have things playing a role in a specific relation. In the schema, we need to specify what role relates to each relation type and who can play this role. Thanks to roles, we are able to guarantee the logical integrity of the data, disallowing marriage between a person and a building, for example. Unless we specifically allow such a relationship in the schema.

***** Rules
Lastly, the Grakn schmea is completed with Graql Rules. Rules are used for a query-time capture of dynamic patterns in the data and performing deduction. Rules are the building blocks of automated reasoning in Grakn.

***** Reserved Keywords

****** Native types
- thing
- entity
- attribute
- relation
- role
- rule

****** Data types
- long
- double
- string
- boolean
- datetime

****** Query commands
- define
- undefine
- match
- get
- insert
- delete
- compute

****** Delete and get query modifiers
- offset
- limit
- group
- sort
- asc
- desc

****** Compute query
- centrality
- using

****** Statement properties
- abstract
- as
- id
- type
- isa
- isa!
- sub
- sub!
- key
- has
- plays
- relates
- value
- regex
- when
- then

****** Operators
- or
- not
- like

****** Literal values
- true
- false

**** Concepts
:LOGBOOK:
- State "DONE"       from "TODO"       [2020-07-27 Mon 20:09]
:END:

***** define
used to develop the schema, which represents the dataset stored in the grakn knowledge graph. We use define to add new entities, relations and rules to the schema.
***** entity
An entity is a thing with a distinct existence in the domain. For example, organisation, location and person. The existence of each of these entities is independent of any other concept in the domain.

- Can assign attribute using the *has* keyword
- Can assign a unique identifier using the *key* keyword.
- An entity can play a role in a relation. We use *plays* keyword followed by the roles model.
- We can define entity to inherit all attributes owned and roles played by another entity.
- Abstract entity. There may be scenarios where a parent entity is only defined for other entities to inherit and under no circumstance, do we expect to have any instances of this parent.

***** relation
A relation describes how 2 or more things are in some way connected to each other. For example, firendship and employment. Each of these relations must relate to roles that are played by something else in the domain. In other words, relations are dependent on the existence of at aleast two other things.

****** Role Players of a relation
- Entites, attributes and even other relations can play a role in a relation. To do this we make use of the plays keyword followed by the role's label.
- A relation can relate to any number for roles and attributes.

****** Assign attributes to relation
- We can assign any number of attributes to a relation. To do so, we use the *has* keyword followed by the attribute's label.
- We can use the *key* keyword followed by the attribute's label.

****** Subtype relation.
- We can define a relation to inherit all attributes owned, and roles related to and played by another relation.
- The ability to subtype relations not only mirror the reality of the dataset as perceived in the real world but also enables automated reasoning using type hierarchies.

****** Abstract relation
Similar to abstract entity there may be scenarios where a parent relation is only defined for other relations to inherit and under no circumstance, do we expect to have any instances of this parent.

***** Attribute
An attribute is a piece of information that determined the property of an element in the domain. For example name, language and age. To define a new attribute we use the sub keyword followed by *attribute*, *value* and then type of desired value.

*IMPORTANT*
/Attributes in Grakn are modeled differently to columns in a relational database.The attribute with the value of, for instance 2020-01-01, exists only once in the database and shared among any number of instances that may own it. this is useful when we need to query the graph for anything that has the start-date attribute with value 2020-01-01. It's important to remember this when performing write operations on instances of an attribute type./

*A concept type can have any number of the same attribute that holds different values* In other words, a concept type has many-to-many relation with its attributes.

****** Restrict attribute's value by Regex
Optionally we specify a regex that the values of an attribute type must conform to. To do this, we use the regex keyword followed by a Java Regex Pattern

****** Owners of an attributte
Entities, relations and even attributes can own one or more attributes of their own. To do this we make use of the *has* keyword followed by the attribute's label.

****** Define an attribute to play a role
An attribute can play a role in a relation. To define the role played by an attribute, we use the plays keyword followed by the role's label.

***** undefine
Used to remove the definition of a type or its association with other types from the schema.

- Undefine an attributes association
- Undefine a relation
- undefine a subtype

**** Rules

***** What is a Rule?
Grakn is capable of querying over data via pre-defined rules. Graql rules look for a given pattern in the dataset and when found, create the given queryable relation (/only for time being of the give transaction/). Automated reasoning provided by rules is performed at query (run) time. Rules not only allow shortening and simplifying commonly-used queries but also enable knowledge discovery and implementation of business logic at the database level.

The rule based reasoning allows automated capture and evolution of patterns with the knowledge grah. Graql reasoning is performed at query time and is guaranteed to be complete. Thanks to the reasoning facility, common patterns in the knowledge graph can be defined and associated with existing schema elements. The association happends by means of tules. This not only allows to compress and simplify typical queries, but offers the ability to derive new non-trivial information by combining defined patterns. Once a query is executed, Graql will not only query the knowledge graph for exact macthes but will also inspect the defined rules tocheck whether additional information can be found(inferred) by combining the patters defined in the rules. The completeness property of Graql reasoning guaranteers that for a given content of the knowledge graph and the defined rule set, the query result shall contain all possible answers derived by combining database lookups and rule applications.

- The facts defined via rules are in general not stored in the knowledge graph. However by defining the rule in the schema, at query time the extra fact will be generated so that we can always know who the siblings are.
- Rules are part of the schema and can be queried the same way as other schema elements.
- Rules like any other schema members can be undefined.
- Rules like any other schema memebers can also be undefined.

***** Functional Interpretation
Another way to look at rules is treat them as functions. In that way, we treat each statement as a function returning either true or false.

*Rules are a powerful tool that allows to reason over the explicitly stored data and produce implicit knowledge at run-time*
*** TODO Pattern
/Query pattern anatomy/


*** TODO Query

* SPARQL (semantic web) ([[https://www.youtube.com/watch?v=e5RPhWIBcY4&list=PLea0WJq13cnDDe8V7eVLReIaOnFztOEAq][link]])

** semantic web
Example: Imagine you want to buy some airline tickets and visit airline website & enter souce & destination & dates and stuff and the website starts to list tickets, prices and times (skyscanner.net). What these websites do in behind is they scan and analyze the webistes html and get retrive the data and present it here. But if host website changes then they have to go back and fix the websites.

What if those websites can share the data and the scemantics of that data over the web for use by applications with a set of standards.

The semantic web sets a set of standards and
- best practices for sharing data
- the semantics of that over the that data over the web by use by application

*** Standards
- The RDF data model.
- The SPARQL query Language.
- The RDF Schema and OWL standards for storing vocabularies and ontologies.

*** Best Practices
- Use URIs to name things
- Use of RDF to model data
- Use SPARQL to query to data.

  They provide guidelines for the creation of an infrastructure for the semantic web

** Concepts
*** OWL
OWL (W3C Ontology language) lets us store valuable bits of meaning so that we can get more out of our data.

example ontology: hasSpouse. A hasSpouse B means B hasSpouse A.
*** URLs, URIs, IRIs and Namepaces
**** URL
Uniform Resource Locator
ex: http://www.learningsparql.com/resource/index.html

server: learningsparql.com, can also be IPs
dir on server: resources
file we are trying to reach: index.html

**** URN
Uniform Resource Names
used for example to identifiy books
ex: urn:isbn:006251587x
/note commonly used/

**** URI
Universal Resource Identifier
- encompasess both URLs and URNs
- most URIs are URLs.
ex: http://xmlns.com/foaf/0.1/Person
foaf onttology ([[https://en.wikipedia.org/wiki/FOAF_(ontology)][wiki]]) (Fried of a friend) is a machine-readable ontology describing firends, their activities  & their realtionships to other people. There is no real webpage in the above link, it redirects to somewhere where we can find some vocabulary about the Person.
**** IRIs
Inernationalized Resource Identifiers
IEFT (Internet engineering task force) released IRIs. These are URIs, but can have extra characters like chinese etc.
**** Namespace
- In XML, what if two sets of elements for two different domains use the same name for 2 different things?
  - example is there can be title for person & book.
- With URI we can have a namespace and then we can define set of names (vocabulary) that we can use for our purposes.
*** Using URLs, URIs & Namespaces.
- The name for the Dublin Core standard set of basic metadata terms is the URI
  - http://purl.org/dc/elements/1.1
- An XML documents main enclosing element often includes the attribute setting
  - xlmns:dc="http://purl.org/dc/elements/1.1"
- To indicate that the dc prefix will stand for the Dublin Core namespace URI in that document
- Imagine that an XML processor found the following element in such a document: <dc:title>Weaving the Web</dc:title>
- It would know that it meant title in the Dublin Core sense (the title of a work)
- Like XML, RDF lets you define a prefix to represent a namespace URI.
*** Resource Description Framework (RDF)
- RDF is a language for the resprentation of resources.
  - A resource can be anything. (sun, moon, man, woman, animal etc)
  - Within the contect of the web relevance is given to web resources, i.e. anything that can be located via URL
  - The basic building blocks is the statement (triple).
**** RDF Overview
- One of the main application is data integration.
- W3C Recommendation since 1998
- RDF is a datamodel
- Orginally used for metadata for web resources, then generalized for other applications
- Universal, machine readable exachange format
- Data structured in graphs (vertices, edges)
**** RD2RDF
[[file:./images/RD2RDF-1.png][Table 2 RDF]]
[[file:./images/RD2RDF-2.png][Subject object predicate]]
- From tuples to triples
- Relational data can be represented as triples
  - Row Key --> Subject
  - Column --> Property (predicate)
  - Value --> Value
    [[file:./images/RD2RDF-3.png][RD2RDF-1]]
    [[file:./images/RD2RDF-4.png][RD2RDF-2]]
- Even if there are lot of data we will just have a lot more triples.
***** Triples
- Triples are the statements about things (resources), using URIs and Literal values.
[[file:./images/triple.png][triple]]
***** Graph
- All triples are graph and if we have plently triples then we can represent them as graph.
- RDF graphs are different from mathematical graphs.
[[file:./images/RDF-Graph-1.png][RDF-Graph-1]]
[[file:./images/RDF-Graph-2.png][RDF-Graph-2]]
[[file:./images/RDF-Graph-3.png][RDF-Graph-3]] using URIs to represent the subject and predicate (property).
- Blank Nodes facilitate existential quantification for an invidiual with certain properties with out naming it.
- Allowed assignments for RDF triple
  - Subject: URI or blank node
  - Predicate: URI
  - Object: URI, blank node or literal
- Node and edge labels should be unambiguous so that the original graph is reconstructable from triple list.
****** Literals
- used to model data values
- representation as strings
- interpretation through datatype
- literals without datatype are treated as strings
- Literals may never be the origin of a node of an RDF graph.
[[file:./images/Literals.png][literals]]
**** RDF formats
Most popular RDF formats
| format     | description                               |
|------------+-------------------------------------------|
| N-Triples  | focuses on simple parsing                 |
| Turtle     | focuses on human readability              |
| Notation 3 | with adavanced features beyond RDF        |
| RDF/XML    | official XML serialization of RDF         |
| RDF/Json   | a proposal for serializing RDF in JSON    |
| Json-LD    | a proposal for expressing RDF in JSON     |
| RDFa       | a mechanism for embedding RDFa in (X)HTML |
**** Saving RDF in Files & Databases.
- The technical term for saving RDF as a string of bytes that can be saved on a disk is serializtion.
- All RDF serializations so far have been text files with different syntaxes used to represent the triples.
- If you need to store a very large number of triples, keeping them as Turtle or RDF/XML in one big text file may not be your best option.
- A system that indexes data and decides with data to load into memory when (i.e a database management system) can be more efficient.
  - There are ways to store RDF in a RDMS such as MySQL or Orcale.
  - The best way is a database manager optimized for RDF triples.
  - We call this a triple store.
    - Open Source
      - Apache Jena
    - Commerical
      - Virtuoso
**** RDF Schema
understanding these things will help understand semantic web better
- RDF Scema
- What OWL does, why it is there?
***** what is a vocabulary?
- set of terms stored using a standard format that people can reuse.
  - A vocabulary of property names typically has its own namespace to make it easier to use it with other sets of data. (URIs, URLs etc)
- Where do we find them?
  - There are standard vocabularies online.
  - There might be websites
  - Ask experts.
- Vocabularies are usually stored using RDF Schema and OWL standards.
***** RDF Schema (RDFS) (RDF Vocabulary Description Language)
- It gives people a way to describe vocabularies. RDF Schema is itself a vocabulary with a schema whose triples declare facts.
  - You describe these vocabularies with RDF.
- RDF Schema lets us define new classes of resources.
- This metadata is just accessible to your SPARQL queries as the data itself. (RDF Schema is data about data, so the term metadata, which is also in triplets)
  - Dublin Core is a standard set of basic metadata.

****** example
 [[http://learningsparql.com/2ndeditionexamples/ex043.ttl][ex-43]],  [[http://learningsparql.com/2ndeditionexamples/ex044.ttl][ex-44]] ,  [[http://learningsparql.com/2ndeditionexamples/ex045.ttl][ex-45]]
#+BEGIN_SRC
@prefix ab:   <http://learningsparql.com/ns/addressbook#> .
@prefix rdf:  <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .

ab:Musician
      rdf:type rdfs:Class ;
      rdfs:label "Musician" ;
      rdfs:comment "Someone who plays a musical instrument" .

ab:MusicalInstrument
      a rdfs:Class ;
      rdfs:label "musical instrument" .

ab:playsInstrument
      rdf:type rdf:Property ;
      rdfs:comment "Identifies the instrument that someone plays" ;
      rdfs:label "plays instrument" ;
      rdfs:domain ab:Musician ;
      rdfs:range ab:MusicalInstrument .

ab:i0432 ab:firstName     "Richard" ;
        ab:lastName       "Mutt" ;
        ab:postalCode     "49345" ;
        ab:city           "Springfield" ;
        ab:homeTel        "(229) 276-5135" ;
        ab:streetAddress  "32 Main St." ;
        ab:region         "Connecticut" ;
        ab:email          "richard49@hotmail.com" ;
        ab:playsInstrument ab:vacuumCleaner .

#+END_SRC
- The author declares an ab: playsInstrument property
- The rdfs: domain property means that if we use this ab: palysInstrument property in a triple, then the subject of the triple is an ab: Musician
- The rdfs: range property means that the object of such a triple is an ab: MusicalInstrument
- In traditional Object-Oriented thinking, if we say "members of class Musician have a playsInstrument property", this means that a member of the Musician class must have a playsInstrumentValue
- RDFS and OWL approach this from the opposite direction: the last 2 triples of ab:playsInstrument tell us that if something has a playsInstrument value, then it's a member of class ab:Musician and the playsInstrument value is a member of class ab:MuscialInstrument.
- *Using semantic web standards, by adding one property to the metatdata about the existing resource ab:i0432, that resource becomes a member of a class that it wasn't a member of before*

***** Advantages
- Ability of RDF resources to become members of class based on their data values has made semantic web technology popular in areas such as medical research and intelligence agencies.
- Researchers can accumulate data with little apparent structure and then see what structure turns out to be there.
- That is, which resources turn out to be members of which classes and what their relationships are.
- RDFS lets you define classes as subclasses of other ones and unlike object-oriented systems also lets us define properties as subproperties of one another.
  - This broadens the possiblities for how you can use SPARQL to retrieve information.
**** OWL
- W3C's Web Ontology Language, abbreviated as OWL.
- OWL builds on RDFS to let you define ontologies.
***** What is an Ontology?
Ontologies are formal definitions of vocabularies that allows us to define complex structures as well as new relationships between vocabulary terms and between members of the clases that you define
- Ontology are collection of triples.
***** Example
 [[http://learningsparql.com/2ndeditionexamples/ex045.ttl][ex-45]]
#+BEGIN_SRC
@prefix ab:   <http://learningsparql.com/ns/addressbook#> .
@prefix rdf:  <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix owl:  <http://www.w3.org/2002/07/owl#> .

ab:i0432
   ab:firstName "Richard" ;
   ab:lastName  "Mutt" ;
   ab:spouse    ab:i9771 .

ab:i8301
   ab:firstName "Craig" ;
   ab:lastName  "Ellis" ;
   ab:patient   ab:i9771 .

ab:i9771
   ab:firstName "Cindy" ;
   ab:lastName  "Marshall" .

ab:spouse
   rdf:type owl:SymmetricProperty ;
   rdfs:comment "Identifies someone's spouse" .

ab:patient
   rdf:type rdf:Property ;
   rdfs:comment "Identifies a doctor's patient" .

ab:doctor
   rdf:type rdf:Property ;
   rdfs:comment "Identifies a doctor treating the named resource" ;
   owl:inverseOf ab:patient .
#+END_SRC

- In the above example we have property "spouse" and we are owl:Symmetric property, meaning works both ways.
- Similarly we have properties "patient" and "doctor", which are inverse of each other.
